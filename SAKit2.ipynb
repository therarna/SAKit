{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb322954-cd9b-4caa-8679-aaf367568f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0acdbb4-ca71-4884-af29-d4a792a1593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nctrldedupfaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-P-2/results/05.Isoform_Novel/STAD_P2.isoform_classification.filtered_lite.dedup.faa\"\\ncasededupfaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/STAD-T1.isoform_classification.filtered_lite.dedup.faa\"\\ncase8aafaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/STAD-T1.isoform_classification.filtered_lite.8.faa\"\\nparse_blast_sh=\"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/parse_blast.sh\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file paths\n",
    "\n",
    "casefaa = \"/home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.faa\"\n",
    "outpath=os.path.dirname(casefaa)\n",
    "prefix=\"COAD_0012934312_CA.isoform_classification.filtered_lite.specific5\"\n",
    "\n",
    "\n",
    "### control of mouse\n",
    "'''\n",
    "ctrl_C1498_faa = \"/home/jovyan/work/02.ResDev/19.USTC_C1498/03.Result_large_scale_variants/C1498_Tail/results/05.Isoform_Novel/C1498_Tail.isoform_classification.filtered_lite.faa\"\n",
    "ctrl_gl261_faa = \"/home/jovyan/work/08.data_GBM/04.Result.large-scale.var/C57P/results/05.Isoform_Novel/C57P.isoform_classification.filtered_lite.faa\"\n",
    "ctrl_B16F1O_faa = \"/home/jovyan/work/02.ResDev/18.USTC_B16F10/Result_B16F10_ctrl/results/05.Isoform_Novel/B16F10_ctrl.isoform_classification.filtered_lite.faa\"\n",
    "'''\n",
    "### control of human\n",
    "COAD_0009689008 = \"/home/jovyan/work/10.data_CODA_ahslyy/0009689008_N/results/05.Isoform_Novel/COAD_0009689008_N.isoform_classification.filtered_lite.faa\"\n",
    "COAD_0012934312 = \"/home/jovyan/work/10.data_CODA_ahslyy/0012934312_N/results/05.Isoform_Novel/COAD_0012934312_N.isoform_classification.filtered_lite.faa\"\n",
    "COAD_0014215947 = \"/home/jovyan/work/10.data_CODA_ahslyy/0014215947_N/results/05.Isoform_Novel/COAD_0014215947_N.isoform_classification.filtered_lite.faa\"\n",
    "COAD_0014274065 = \"/home/jovyan/work/10.data_CODA_ahslyy/0014274065_N/results/05.Isoform_Novel/COAD_0014274065_N.isoform_classification.filtered_lite.faa\"\n",
    "\n",
    "\n",
    "sp_human_faa = \"/home/jovyan/work/00.database/18.uniprot/sp_human_canon_isoform.fasta\"\n",
    "tr_human_faa = \"/home/jovyan/work/00.database/18.uniprot/tr_human_canon_isoform.fasta\"\n",
    "sp_mouse_faa = \"/home/jovyan/work/00.database/18.uniprot/sp_mouse_canon_isoform.fasta\"\n",
    "tr_mouse_faa = \"/home/jovyan/work/00.database/18.uniprot/tr_mouse_canon_isoform.fasta\"\n",
    "\n",
    "ideb_select_faa = os.path.join(outpath,\"\".join([prefix,\".iedb.faa\"]))\n",
    "case_sepecific_faa1 = casefaa.split(\".faa\")[0]+\".specific1.faa\"\n",
    "case_sepecific_faa2 = casefaa.split(\".faa\")[0]+\".specific2.faa\"\n",
    "case_sepecific_faa3 = casefaa.split(\".faa\")[0]+\".specific3.faa\"\n",
    "case_sepecific_faa4 = casefaa.split(\".faa\")[0]+\".specific4.faa\"\n",
    "case_sepecific_faa5 = casefaa.split(\".faa\")[0]+\".specific5.faa\"\n",
    "case_sepecific_faa6 = casefaa.split(\".faa\")[0]+\".specific6.faa\"\n",
    "\n",
    "'''\n",
    "ctrldedupfaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-P-2/results/05.Isoform_Novel/STAD_P2.isoform_classification.filtered_lite.dedup.faa\"\n",
    "casededupfaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/STAD-T1.isoform_classification.filtered_lite.dedup.faa\"\n",
    "case8aafaa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/STAD-T1.isoform_classification.filtered_lite.8.faa\"\n",
    "parse_blast_sh=\"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/parse_blast.sh\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71a2c6-0afd-4364-a361-2b4a7a99407a",
   "metadata": {},
   "source": [
    "## 01.find cancer-specific protein sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a8bf1b-a8e3-4843-a752-6fddd5dc186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_sequences(ctrl_faa, case_faa, out_faa, window_size=13):\n",
    "    # 读取ctrl_faa序列\n",
    "    print(\"正在读取对照FASTA文件...\")\n",
    "    ctrl_sequences = set()\n",
    "    for record in SeqIO.parse(ctrl_faa, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        for i in range(len(seq) - window_size + 1):\n",
    "            ctrl_sequences.add(seq[i:i + window_size])\n",
    "\n",
    "    # 从case_faa中找出不存在于ctrl_faa的序列\n",
    "    print(\"正在从实验组FASTA文件中提取唯一的多肽序列...\")\n",
    "    unique_peptides = defaultdict(list)\n",
    "    for record in SeqIO.parse(case_faa, \"fasta\"):\n",
    "        seq = str(record.seq)\n",
    "        read_id = record.id\n",
    "        for i in range(len(seq) - window_size + 1):\n",
    "            peptide = seq[i:i + window_size]\n",
    "            if peptide not in ctrl_sequences:\n",
    "                unique_peptides[read_id].append((peptide, i))\n",
    "\n",
    "    # 合并同一read_id下的相邻多肽\n",
    "    print(\"正在合并同一read_id下的相邻多肽序列...\")\n",
    "    merged_peptides = []\n",
    "    for read_id, peptides in unique_peptides.items():\n",
    "        merged = []\n",
    "        peptides.sort(key=lambda x: x[1])  # 按坐标排序\n",
    "        for peptide, start in peptides:\n",
    "            if not merged or start > merged[-1][2]:\n",
    "                merged.append((peptide, start, start + len(peptide)))\n",
    "            else:\n",
    "                overlap_start = merged[-1][2] - start\n",
    "                merged_seq = merged[-1][0][:-overlap_start] + peptide\n",
    "                merged[-1] = (merged_seq, merged[-1][1], start + len(peptide))\n",
    "\n",
    "        merged_peptides.extend([(f\">{read_id}_{start}_{end}\", seq) for seq, start, end in merged])\n",
    "\n",
    "    # 写入out_faa\n",
    "    print(f\"正在将合并后的多肽序列写入文件 {out_faa}...\")\n",
    "    with open(out_faa, \"w\") as out_file:\n",
    "        for header, sequence in merged_peptides:\n",
    "            out_file.write(f\"{header}\\n{sequence}\\n\")\n",
    "    print(f\"写入完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4b1121-143d-4317-b318-11f149e8a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific1.faa...\n",
      "写入完成！\n",
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific2.faa...\n",
      "写入完成！\n",
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific3.faa...\n",
      "写入完成！\n",
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific4.faa...\n",
      "写入完成！\n",
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific5.faa...\n",
      "写入完成！\n",
      "正在读取对照FASTA文件...\n",
      "正在从实验组FASTA文件中提取唯一的多肽序列...\n",
      "正在合并同一read_id下的相邻多肽序列...\n",
      "正在将合并后的多肽序列写入文件 /home/jovyan/work/10.data_CODA_ahslyy/03.Result.SAKit2_corrected/0012934312_CA/results/05.Isoform_Novel/COAD_0012934312_CA.isoform_classification.filtered_lite.specific6.faa...\n",
      "写入完成！\n"
     ]
    }
   ],
   "source": [
    "# 示例用法\n",
    "find_unique_sequences(COAD_0009689008, casefaa, case_sepecific_faa1)\n",
    "find_unique_sequences(COAD_0012934312, case_sepecific_faa1, case_sepecific_faa2)\n",
    "find_unique_sequences(COAD_0014215947, case_sepecific_faa2, case_sepecific_faa3)\n",
    "find_unique_sequences(COAD_0014274065, case_sepecific_faa3, case_sepecific_faa4)\n",
    "find_unique_sequences(sp_human_faa, case_sepecific_faa4, case_sepecific_faa5)\n",
    "find_unique_sequences(tr_human_faa, case_sepecific_faa5, case_sepecific_faa6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5259e9f-edb2-402c-bf6b-8947fb6b40a8",
   "metadata": {},
   "source": [
    "## 02. Select from IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9126ebea-d2af-42cf-ac29-cae0bcf94426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def select_epitope_from_IDEB(iedb_epitope, aa_file, aa_result):\n",
    "    # 读取 file1.csv,获取所有表位序列\n",
    "    epitopes = []\n",
    "    with open(iedb_epitope, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # 跳过标题行\n",
    "        for row in reader:\n",
    "            epitope = row[2]  # 假设第三列是表位序列\n",
    "            if len(epitope) >= 7:\n",
    "                epitopes.append(epitope)\n",
    "\n",
    "    # 读取 file2.fasta,查找包含表位序列的蛋白质\n",
    "    result = []\n",
    "    for record in SeqIO.parse(aa_file, 'fasta'):\n",
    "        sequence = str(record.seq)\n",
    "        contained_epitopes = []\n",
    "        for epitope in epitopes:\n",
    "            if epitope in sequence:\n",
    "                contained_epitopes.append(epitope)\n",
    "\n",
    "        new_seq = Seq(sequence)  # 创建新的 Seq 对象\n",
    "        if contained_epitopes:\n",
    "            new_description = f\"IEDB_epitope: {', '.join(contained_epitopes)}\"\n",
    "        else:\n",
    "            new_description = \"IEDB_epitope: null\"\n",
    "\n",
    "        new_record = SeqRecord(new_seq, id=record.id, description=new_description)\n",
    "        result.append(new_record)\n",
    "\n",
    "    # 将结果写入新的 fasta 文件\n",
    "    with open(aa_result, 'w') as output_file:\n",
    "        SeqIO.write(result, output_file, 'fasta-2line')\n",
    "\n",
    "# 示例用法\n",
    "iedb_epitope = \"/home/jovyan/work/02.ResDev/22.epitope_from_idedb/CEDAR.IEDB/epitope_human_v3.csv\"\n",
    "aa_file = \"/home/jovyan/work/10.data_CODA_ahslyy/0014274065_CA/results/09.EpitopePrediction/01.protein_sequence/COAD_0014274065.manual.AminoAcidseq.faa\"\n",
    "aa_result = \".\".join([aa_file[:-4],\"iedb\",\"faa\"])\n",
    "select_epitope_from_IDEB(iedb_epitope, aa_file, aa_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca256c1-4500-4ae4-b4d2-3a0f44413563",
   "metadata": {},
   "source": [
    "## 03.溯源回转录本异构体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e603db-7875-4a3b-98d1-edb3e7834cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sequence(sequence):\n",
    "    return sequence[::-1]\n",
    "\n",
    "def check_trans_state(exon_start,exon_end,remain_protein_seq_len):\n",
    "    #print(\"exon_start,exon_end,remain_protein_seq_len\",exon_start,exon_end,remain_protein_seq_len)\n",
    "    exon_amino_length = (exon_end - exon_start)//3\n",
    "    if exon_amino_length < remain_protein_seq_len:\n",
    "        current_acid_index = exon_end\n",
    "        remain_protein_seq_len -= exon_amino_length\n",
    "        trans_started = True\n",
    "        trans_overed = False\n",
    "        \n",
    "    elif  exon_amino_length == remain_protein_seq_len:\n",
    "        current_acid_index = exon_end\n",
    "        remain_protein_seq_len = 0 \n",
    "        trans_started = True\n",
    "        trans_overed = True \n",
    "        \n",
    "    elif exon_amino_length > remain_protein_seq_len:\n",
    "        current_acid_index = exon_start + remain_protein_seq_len*3\n",
    "        remain_protein_seq_len = 0\n",
    "        trans_started = True\n",
    "        trans_overed = True \n",
    "    #print(exon_start, exon_end, remain_protein_seq_len,current_acid_index)\n",
    "    return ([remain_protein_seq_len,current_acid_index,trans_started,trans_overed])\n",
    "\n",
    "def find_amino_acid_position(exon_coordinates, translation_end, remain_protein_seq_len,strand):\n",
    "    # 计算翻译的起始密码子在基因组上的起始位置\n",
    "    translation_end_position = translation_end\n",
    "    #print(\"remain_protein_seq_len\",remain_protein_seq_len)\n",
    "    #print(\"translation_end\",translation_end)\n",
    "    \n",
    "    # 遍历exons\n",
    "    trans_started = False\n",
    "    trans_overed = False\n",
    "    for exon_start, exon_end in exon_coordinates:\n",
    "        #print(translation_end_position,exon_start, exon_end)\n",
    "        exon_length = exon_end - exon_start + 1\n",
    "        # 如果翻译的起始密码子在当前exon内\n",
    "        \n",
    "        if (trans_started == False) and (trans_overed == False):\n",
    "            if translation_end_position in range(exon_start, exon_end):\n",
    "                remain_protein_seq_len,current_acid_index,trans_started,trans_overed = \\\n",
    "                    check_trans_state(translation_end_position, exon_end, remain_protein_seq_len)\n",
    "                #print(\"current_acid_index\",current_acid_index,\"trans_started\",trans_started,\"trans_overed\",trans_overed,\"remain_protein_seq_len\",remain_protein_seq_len)\n",
    "            else:\n",
    "                print(exon_start, exon_end, translation_end_position)\n",
    "                remain_protein_seq_len,current_acid_index,trans_started,trans_overed = \\\n",
    "                    check_trans_state(translation_end_position, exon_end, remain_protein_seq_len)\n",
    "        elif (trans_started == True) and (trans_overed == False):\n",
    "            remain_protein_seq_len,current_acid_index,trans_started,trans_overed = \\\n",
    "                    check_trans_state(exon_start, exon_end,remain_protein_seq_len)\n",
    "            \n",
    "        elif  (trans_started == True) and (trans_overed == True):\n",
    "            break\n",
    "    #print(current_acid_index)\n",
    "    return current_acid_index\n",
    "\n",
    "    \n",
    "def find_isoform_info(query_seq, info,exon_coordinates):\n",
    "    orf_seq = info[\"ORF_seq\"]\n",
    "    strand = info[\"strand\"]\n",
    "    cds_genomic_start = int(info[\"CDS_genomic_start\"])\n",
    "    cds_genomic_end = int(info[\"CDS_genomic_end\"])\n",
    "\n",
    "    #print(orf_seq,query_seq,strand,cds_genomic_start,cds_genomic_end)\n",
    "    reverse_query_seq = reverse_sequence(query_seq)\n",
    "    reverse_orf_seq = reverse_sequence(orf_seq)\n",
    "    pep_end_pos = reverse_orf_seq.find(reverse_query_seq)\n",
    "    pep_start_pos =  pep_end_pos+len(reverse_query_seq)\n",
    "    '''\n",
    "    # 由于原始文件中cds_genomic_start是以起始密码子为起始为点的，\n",
    "    # 但是由于5'的降解，给出的蛋白质序列是认为更加完整的蛋白质序列，例如GBM_GL261细胞系检出的PB.1624.1\n",
    "        KFCSYIFTNGKTKTLRGGVIVTGNRLGTLVQTYVNAINSGTVPCLENAVTTLAQRENSIAVQKAADHY\n",
    "        SEQMAQRVRLPTDTLQELLTVHAACEKEAIAVFMEHSFKDDEQEFQKKLVVTIEERKEEFIRQNEAAS\n",
    "        IRHCQAELERLSESLRKSISCGAFSVPGGHSLYLEARKKIELGYQQVPRKGVKAKEVLKSFLQSQAIM\n",
    "        EDSILQSDKALTDGERAIAAERTKKEVAEKELELLRQRQKEQEQVMEAQERSFRENIAKLQEKMESEK\n",
    "        EMLLREQEKMLEHKLKVQEELLIEGFREKSDMLKNEISHLREEMERTRRKPSLFGQILDTVGNAFIMI\n",
    "        LPGAGKLFGVGLKFLGSLSS\n",
    "    它的第一个M是在第71个氨基酸位置，它给的cds_genomic_start是从第71个M氨基酸开始的，所以不用cds_genomic_start，\n",
    "    而是采用从翻译的终点溯源起点的算法.\n",
    "    pep_start_pos 是query_seq（peptide） 在orf_seq（蛋白质序列）的上游位置，也就是在正向翻译的下游位置\n",
    "    pep_end_pos 是query_seq（peptide） 在orf_seq（蛋白质序列）的下游位置，也就是在正向翻译的上游位置\n",
    "    '''\n",
    "    if strand == \"+\":\n",
    "        exon_coordinates =  [[-exon_end, -exon_start] for [exon_start,exon_end] in reverse_sequence(exon_coordinates)]\n",
    "        \n",
    "        '''\n",
    "        exon_coordinates 进行了取负反向，使得其翻译方向也变成5'-3'\n",
    "        例如这样的PB.13x的exon坐标 [[4000, 4699], [6000, 6040], [7010, 7023], [9000, 10000]]，假如它是“-”则从右往左，\n",
    "        它的转向取负后：           [[-10000, -9000], [-7023, -7010], [-6040, -6000], [-4699, -4000]]\n",
    "        这样它“+”链都能使用check_trans_state函数\n",
    "        '''\n",
    "        cds_genomic_end = -1 * cds_genomic_end\n",
    "        '''\n",
    "        cds_genomic_end 是在基因组中的翻译终止坐标，在反向坐标,后面要再反向回来。\n",
    "        '''\n",
    "        \n",
    "        cds_genomic_start_speci = -1 * find_amino_acid_position(exon_coordinates, cds_genomic_end, pep_start_pos, -1)\n",
    "        cds_genomic_end_speci = -1 * find_amino_acid_position(exon_coordinates, cds_genomic_end, pep_end_pos, -1)\n",
    "\n",
    "    elif strand == \"-\":\n",
    "\n",
    "        cds_genomic_end =  cds_genomic_end\n",
    "\n",
    "        cds_genomic_end_speci =  find_amino_acid_position(exon_coordinates, cds_genomic_end, pep_end_pos, 1)\n",
    "        cds_genomic_start_speci = find_amino_acid_position(exon_coordinates, cds_genomic_end, pep_start_pos, 1)\n",
    "            \n",
    "    return info, cds_genomic_start_speci, cds_genomic_end_speci, query_seq\n",
    "\n",
    "def parse_gtf_file(file_path):\n",
    "    gtf_dict = {}\n",
    "    with open(file_path, 'r') as gtf_file:\n",
    "        for line in gtf_file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            fields = line.strip().split('\\t')\n",
    "            chrom, source, feature, start, end, score, strand, frame, attributes = fields\n",
    "            isoform_id = attributes.split('\"')[1]\n",
    "            if isoform_id in gtf_dict.keys():\n",
    "                gtf_dict[isoform_id].append([int(start), int(end)])\n",
    "            else:\n",
    "                gtf_dict[isoform_id] = []\n",
    "                \n",
    "    return gtf_dict\n",
    "\n",
    "def process_files(faa_file, isoform_info_file, isoform_gtf, output_file):\n",
    "\n",
    "    ## 解析isoform_classcical.txt 文件\n",
    "    isoform_info = {}\n",
    "    with open(isoform_info_file, \"r\") as file:\n",
    "        header = file.readline().strip().split(\"\\t\")\n",
    "        for line in file:\n",
    "            fields = line.strip().split(\"\\t\")\n",
    "            if fields[45] != \"ORF_seq\" and fields[45] != \"NA\":\n",
    "                isoform_id = fields[0]\n",
    "                isoform_info[isoform_id] = dict(zip(header, fields))\n",
    "    #print(isoform_info.keys())\n",
    "\n",
    "    ##解析gtf文件\n",
    "    gtf_dict = parse_gtf_file(isoform_gtf)\n",
    "    \n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        out_file.write(\"\\t\".join(header + [\"read_id\", \"CDS_genomic_start_common\", \"CDS_genomic_end_common\", \"sequence_common\"]) + \"\\n\")\n",
    "        for record in SeqIO.parse(faa_file, \"fasta\"):\n",
    "            read_id = record.id.split(\"_\")[0]\n",
    "            read_seq = str(record.seq)\n",
    "            if read_id in isoform_info:  # Check if read_id exists in isoform_info\n",
    "                read_seq = str(record.seq)\n",
    "                info = isoform_info[read_id]\n",
    "                exon_coordinates = gtf_dict[read_id]\n",
    "                #print(exon_coordinates)\n",
    "                result_info, cds_genomic_start_speci, cds_genomic_end_speci, sequence_speci = find_isoform_info(read_seq, info, exon_coordinates)\n",
    "                out_file.write(\"\\t\".join([str(result_info[col]) for col in header] + [read_id, str(cds_genomic_start_speci), str(cds_genomic_end_speci), sequence_speci]) + \"\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: No information available for {read_id}\")  # Optionally, log the missing read_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f210fe9a-b23e-447f-a518-721769da4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40191553 -40189653 -40189229\n",
      "-40191553 -40189653 -40189229\n",
      "-223775836 -223774834 -223769975\n",
      "-223775836 -223774834 -223769975\n",
      "225845536 225846932 225855909\n",
      "225845536 225846932 225855909\n",
      "73007217 73008292 73043934\n",
      "73007217 73008292 73043934\n",
      "-119942143 -119940586 -119933010\n",
      "-119942143 -119940586 -119933010\n",
      "10797050 10797881 10799990\n",
      "10797050 10797881 10799990\n",
      "10797050 10797881 10799989\n",
      "10797050 10797881 10799989\n",
      "-34663279 -34658832 -34651623\n",
      "-34663279 -34658832 -34651623\n",
      "-49764740 -49762873 -49758953\n",
      "-49764740 -49762873 -49758953\n",
      "56176808 56178093 56178780\n",
      "56176808 56178093 56178780\n",
      "-109486603 -109486471 -109483665\n",
      "-109486603 -109486471 -109483665\n",
      "74848244 74848701 74853944\n",
      "74848244 74848701 74853944\n",
      "-53491645 -53490130 -53465575\n",
      "-53491645 -53490130 -53465575\n",
      "-41770902 -41770802 -41766267\n",
      "-41770902 -41770802 -41766267\n",
      "-15631099 -15630776 -15627163\n",
      "-15631099 -15630776 -15627163\n",
      "-86138188 -86137469 -86127380\n",
      "-86138188 -86137469 -86127380\n",
      "-108474008 -108472827 -108470709\n",
      "-108474008 -108472827 -108470709\n",
      "165747811 165748903 165758664\n",
      "165747811 165748903 165758664\n",
      "-172506277 -172504091 -172489593\n",
      "-172506277 -172504091 -172489593\n",
      "-172506277 -172504091 -172489593\n",
      "-172506277 -172504091 -172489593\n",
      "47210413 47210897 47246281\n",
      "47210413 47210897 47246281\n",
      "15012448 15014466 15043510\n",
      "15012448 15014466 15043510\n",
      "-37976646 -37975709 -37969838\n",
      "-37976646 -37975709 -37969838\n",
      "-50272726 -50271361 -50270636\n",
      "-50272726 -50271361 -50270636\n",
      "-108694033 -108693360 -108690898\n",
      "-108694033 -108693360 -108690898\n",
      "82818661 82819253 82842162\n",
      "82818661 82819253 82842162\n",
      "-72904206 -72903709 -72897172\n",
      "-72904206 -72903709 -72897172\n",
      "36315761 36317391 36322299\n",
      "36315761 36317391 36322299\n",
      "36315761 36317391 36322299\n",
      "36315761 36317391 36322299\n",
      "-6402537 -6402316 -6398825\n",
      "-6402537 -6402316 -6398825\n",
      "75186105 75186527 75187224\n",
      "75186105 75186527 75187224\n",
      "133097814 133098762 133102888\n",
      "133097814 133098762 133102888\n",
      "313981 315478 340721\n",
      "313981 315478 340721\n",
      "85276898 85279426 85305804\n",
      "85276898 85279426 85305804\n",
      "-100840935 -100840714 -100826802\n",
      "-100840935 -100840714 -100826802\n"
     ]
    }
   ],
   "source": [
    "fasta_file = case_sepecific_faa6\n",
    "isoform_info_file = \"_\".join([casefaa.split(\".faa\")[0],\"classification.txt\"])\n",
    "isoform_gtf = \".\".join([casefaa.split(\".faa\")[0],\"gtf\"])\n",
    "output_file = \".\".join([casefaa.split(\".faa\")[0],\"common_append.txt\"])\n",
    "process_files(fasta_file, isoform_info_file, isoform_gtf, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3037d-06ed-42a3-923c-b185006a6dfc",
   "metadata": {},
   "source": [
    "## 04.查找多样本间共享表位（备用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7165287f-ef49-4b17-a922-9fd32e5ba6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义一个函数用于将序列切分成窗口\n",
    "def sliding_window(sequence, window_size):\n",
    "    for i in range(len(sequence) - window_size + 1):\n",
    "        yield sequence[i:i+window_size], i\n",
    "\n",
    "# 读取fasta文件,并进行滑窗处理,对于相同的多肽序列只保留一个\n",
    "def process_fasta_file(file_path, window_size=8):\n",
    "    sequences = defaultdict(list)\n",
    "    seen_peptides = set()\n",
    "\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequence_id = record.id\n",
    "        sequence = str(record.seq)\n",
    "\n",
    "        if not sequence:\n",
    "            continue\n",
    "\n",
    "        for peptide, start in sliding_window(sequence, window_size):\n",
    "            if peptide not in seen_peptides:\n",
    "                seen_peptides.add(peptide)\n",
    "                sequences[peptide].append((sequence_id, start, len(sequence)))  # 添加序列长度信息\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# 找到在所有样本中共同出现的多肽序列\n",
    "def find_common_peptides(all_sequences):\n",
    "    common_peptides = set.intersection(*[set(sequences.keys()) for sequences in all_sequences])\n",
    "    return common_peptides\n",
    "\n",
    "# 根据共同的多肽序列构建每个样本的common_sequences\n",
    "def build_common_sequences(all_sequences, common_peptides):\n",
    "    common_sequences = []\n",
    "    for sequences in all_sequences:\n",
    "        common_seq = defaultdict(list)\n",
    "        for peptide in common_peptides:\n",
    "            if peptide in sequences:\n",
    "                common_seq[peptide] = sequences[peptide]\n",
    "            else:\n",
    "                print(\"error: not in common_seq --> \"+peptide)\n",
    "        common_sequences.append(common_seq)\n",
    "        #print(common_sequences)\n",
    "    return common_sequences\n",
    "\n",
    "# 合并来自同一个序列的多肽序列\n",
    "def merge_sequences_by_id(sample_comm_sequences):\n",
    "    #print(sample_comm_sequences)\n",
    "    merged_sequences = []\n",
    "    merged = defaultdict(list)\n",
    "    \n",
    "    for peptide, occurrences in sample_comm_sequences.items():\n",
    "        #print(peptide, occurrences)\n",
    "        sequence_id, start, length = occurrences[0]\n",
    "        merged[sequence_id].append((peptide, start, length))\n",
    "\n",
    "    for sequence_id, peptides in merged.items():\n",
    "        merged_peptides = merge_peptides_by_coordinates(peptides, max_distance=8)\n",
    "        merged_sequences.append((sequence_id, merged_peptides))\n",
    "\n",
    "    return merged_sequences\n",
    "\n",
    "# 合并多肽序列根据坐标\n",
    "def merge_peptides_by_coordinates2(peptides, max_distance):\n",
    "    merged_peptides = []\n",
    "    peptides.sort(key=lambda x: x[1])  # 按启始坐标排序\n",
    "\n",
    "    current_merged = []\n",
    "    for peptide, start, length in peptides:\n",
    "        if not current_merged:\n",
    "            current_merged.append((peptide, start, start + len(peptide)))\n",
    "        else:\n",
    "            last_peptide, last_start, last_end = current_merged[-1]\n",
    "            if start < last_end:\n",
    "                overlap = last_end - start\n",
    "                current_merged[-1] = (last_peptide[:-overlap] + peptide, last_start, start + len(peptide))\n",
    "            else:\n",
    "                current_merged.append((peptide, start, start + len(peptide)))\n",
    "\n",
    "    merged_peptides = [''.join(seq for seq, _, _ in current_merged)]\n",
    "    return merged_peptides\n",
    "    \n",
    "def merge_peptides_by_coordinates(peptides, max_distance):\n",
    "    merged_peptides = []\n",
    "    peptides.sort(key=lambda x: x[1])  # 按启始坐标排序\n",
    "\n",
    "    current_merged = []\n",
    "    for peptide, start, length in peptides:\n",
    "        if not current_merged:\n",
    "            current_merged.append((peptide, start, start + len(peptide)))\n",
    "        else:\n",
    "            last_peptide, last_start, last_end = current_merged[-1]\n",
    "            if start < last_end:\n",
    "                overlap = last_end - start\n",
    "                current_merged[-1] = (last_peptide[:-overlap] + peptide, last_start, start + len(peptide))\n",
    "            else:\n",
    "                current_merged.append((peptide, start, start + len(peptide)))\n",
    "\n",
    "    merged_peptides = current_merged\n",
    "    return merged_peptides\n",
    "\n",
    "# 输出合并后的序列到fasta文件\n",
    "def write_merged_sequences2(merged_sequences, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i, (sequence_id, merged_peptides) in enumerate(merged_sequences, start=1):\n",
    "            for j, (peptide, start, end) in enumerate(merged_peptides, start=1):\n",
    "                read_id = f\"{sequence_id}_{i}_{j}\"\n",
    "                f.write(f\">{read_id}\\n{peptide}\\n\")\n",
    "def write_merged_sequences(merged_sequences, output_file):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for i, (sequence_id, merged_peptides) in enumerate(merged_sequences, start=1):\n",
    "            for j, (peptide, start, end) in enumerate(merged_peptides, start=1):\n",
    "                read_id = f\"{sequence_id}_{i}_{j}_{start}_{end}\"\n",
    "                f.write(f\">{read_id}\\n{peptide}\\n\")\n",
    "\n",
    "# 主函数\n",
    "def main(fasta_files, window_size=8, output_file=\"merged.fasta\"):\n",
    "    all_sequences = [process_fasta_file(file_path, window_size) for file_path in fasta_files]\n",
    "    common_peptides = find_common_peptides(all_sequences)\n",
    "    #print(common_peptides)\n",
    "    common_sequences = build_common_sequences(all_sequences, common_peptides)\n",
    "    #print(common_sequences[1])\n",
    "    merged_sequences = [merge_sequences_by_id(sample_comm_sequences) for sample_comm_sequences in common_sequences]\n",
    "    #print(merged_sequences)\n",
    "    for i, sequences in enumerate(merged_sequences, start=1):\n",
    "        output_file_path = f\"{output_file.rsplit('.', 1)[0]}_{i}.fasta\"\n",
    "        write_merged_sequences(sequences, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d099f07-f282-442d-aa6c-8d379547499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个case样本中查找shared的neo protein\n",
    "#case1faa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-1/results/05.Isoform_Novel/STAD-T1.isoform_classification.filtered_lite.specific4.faa\"\n",
    "#case2faa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-2/results/05.Isoform_Novel/STAD_T2.isoform_classification.filtered_lite.specific4.faa\"\n",
    "#case3faa = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/CCSW-01-T-3/results/05.Isoform_Novel/STAD_T3.isoform_classification.filtered_lite.specific4.faa\"\n",
    "#outpath2 = \"/home/jovyan/work/09.data_STAD_sdfyy/04.Result.LSTVs/\"\n",
    "#fasta_files = [case1faa, case2faa, case3faa]\n",
    "#main(fasta_files, output_file=os.path.join(outpath2, \"merged.fasta\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
